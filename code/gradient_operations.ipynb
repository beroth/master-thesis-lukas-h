{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:17:25.121174Z",
     "start_time": "2024-11-08T09:17:25.103128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# google colab related stuff\n",
    "def is_running_on_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "if is_running_on_colab():\n",
    "    import os\n",
    "    os.chdir(\"/content\")\n",
    "    !git clone https://github.com/lukas-hinterleitner/master-thesis.git\n",
    "    \n",
    "    os.chdir(\"/content/master-thesis\")\n",
    "    !git submodule init\n",
    "    !git submodule update\n",
    "    \n",
    "    !pip uninstall ibis-framework torchvision torchaudio -y\n",
    "    !pip install -r requirements.txt\n",
    "    os.kill(os.getpid(), 9)"
   ],
   "id": "80ad9f31be3b471e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## !!! \n",
    "## Keep in mind, that the google colab session will crash after executing the above cell. This is necessary to load open-instruct as an editable package. Just continue by executing the next cell.\n",
    "## !!!"
   ],
   "id": "5a0b00095f9ef836"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:17:38.706040Z",
     "start_time": "2024-11-08T09:17:38.696205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# google colab related stuff\n",
    "def is_running_on_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "if is_running_on_colab():\n",
    "    import os\n",
    "    os.chdir(\"/content/master-thesis/code\")\n",
    "    \n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ],
   "id": "829271f117d99062",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-08T09:18:00.301555Z",
     "start_time": "2024-11-08T09:17:39.944553Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from config import hf_model_id, lima_filtered_paraphrased_dataset_path, get_dataset_config\n",
    "\n",
    "from utilities.preprocessing import prepare_dataset\n",
    "from utilities.gradient_operations import get_gradients, get_flattened_weight_vector"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-08 10:17:55,028] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:gcc -pthread -B /home/lukashinterleitner/anaconda3/envs/master-thesis/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/lukashinterleitner/anaconda3/envs/master-thesis/include -fPIC -O2 -isystem /home/lukashinterleitner/anaconda3/envs/master-thesis/include -fPIC -c /tmp/tmp583zpovg/test.c -o /tmp/tmp583zpovg/test.o\n",
      "INFO:root:gcc -pthread -B /home/lukashinterleitner/anaconda3/envs/master-thesis/compiler_compat /tmp/tmp583zpovg/test.o -laio -o /tmp/tmp583zpovg/a.out\n",
      "/home/lukashinterleitner/anaconda3/envs/master-thesis/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "INFO:root:gcc -pthread -B /home/lukashinterleitner/anaconda3/envs/master-thesis/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/lukashinterleitner/anaconda3/envs/master-thesis/include -fPIC -O2 -isystem /home/lukashinterleitner/anaconda3/envs/master-thesis/include -fPIC -c /tmp/tmpgm60n2fc/test.c -o /tmp/tmpgm60n2fc/test.o\n",
      "INFO:root:gcc -pthread -B /home/lukashinterleitner/anaconda3/envs/master-thesis/compiler_compat /tmp/tmpgm60n2fc/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmpgm60n2fc/a.out\n",
      "/home/lukashinterleitner/anaconda3/envs/master-thesis/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:18:00.327879Z",
     "start_time": "2024-11-08T09:18:00.315042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "id": "af748f64e20a4fa8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:18:00.734386Z",
     "start_time": "2024-11-08T09:18:00.395319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "cdeb08b4f6d2957",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:18:01.970950Z",
     "start_time": "2024-11-08T09:18:00.841768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, is_decoder=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)"
   ],
   "id": "addc6245190d0ee2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50279\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:18:02.061482Z",
     "start_time": "2024-11-08T09:18:02.051125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_config = get_dataset_config(model)\n",
    "dataset_config"
   ],
   "id": "2a785ac31470274e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetConfig(chat_template='tulu', preference_chosen_key='chosen', preference_rejected_key='rejected', sft_messages_key='messages', binary_messages_key='messages', label='binary_labels', convert_preference_to_binary_dataset=False, max_token_length=2048, max_prompt_token_length=None, sanity_check=False, sanity_check_max_samples=100, batched=False, load_from_cache_file=True, num_proc=12, train_only_on_prompt=True, ncols=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:18:02.177888Z",
     "start_time": "2024-11-08T09:18:02.166886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_gpu = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "device"
   ],
   "id": "c64e2b22c156b3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:50.537620Z",
     "start_time": "2024-11-08T09:27:50.515749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "model.eval() # set to evaluation because we don't need to update weights"
   ],
   "id": "f46f61e1b7ba1240",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OlmoForCausalLM(\n",
       "  (model): OlmoModel(\n",
       "    (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x OlmoDecoderLayer(\n",
       "        (self_attn): OlmoSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): OlmoRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): OlmoMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): OlmoLayerNorm()\n",
       "        (post_attention_layernorm): OlmoLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): OlmoLayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:51.749582Z",
     "start_time": "2024-11-08T09:27:51.738389Z"
    }
   },
   "cell_type": "code",
   "source": "model.num_parameters()",
   "id": "17e45fe6817a215",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176764416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:52.154224Z",
     "start_time": "2024-11-08T09:27:52.115392Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_from_disk(lima_filtered_paraphrased_dataset_path)",
   "id": "b94aff18b2d7ddec",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:52.630392Z",
     "start_time": "2024-11-08T09:27:52.619375Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.column_names",
   "id": "3eb3e15ea55c0ab1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'messages', 'paraphrased_messages']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:54.085862Z",
     "start_time": "2024-11-08T09:27:54.070165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create two datasets for original messages and paraphrased messages\n",
    "\n",
    "original_dataset_columns = [\"id\", \"messages\"]\n",
    "paraphrased_dataset_columns = [\"id\", \"paraphrased_messages\"]\n",
    "\n",
    "original_dataset = dataset.select_columns(original_dataset_columns)\n",
    "paraphrased_dataset = dataset.select_columns(paraphrased_dataset_columns)"
   ],
   "id": "80d113df7deefae7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:54.796663Z",
     "start_time": "2024-11-08T09:27:54.788880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(original_dataset.column_names)\n",
    "print(paraphrased_dataset_columns)"
   ],
   "id": "c9130c45e5829b44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'messages']\n",
      "['id', 'paraphrased_messages']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:55.240181Z",
     "start_time": "2024-11-08T09:27:55.229602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# rename paraphrased messages to messages since open-instruct encode_sft_example only works with 'messages'key\n",
    "paraphrased_dataset = paraphrased_dataset.rename_column(\"paraphrased_messages\", \"messages\")"
   ],
   "id": "e52b95d5fd142f69",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:56.186873Z",
     "start_time": "2024-11-08T09:27:56.177231Z"
    }
   },
   "cell_type": "code",
   "source": "original_dataset",
   "id": "45c7b281507a435b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages'],\n",
       "    num_rows: 988\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:27:57.630392Z",
     "start_time": "2024-11-08T09:27:57.620799Z"
    }
   },
   "cell_type": "code",
   "source": "paraphrased_dataset",
   "id": "37fbaaaf4b0f9c40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages'],\n",
       "    num_rows: 988\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:28:00.116584Z",
     "start_time": "2024-11-08T09:28:00.109808Z"
    }
   },
   "cell_type": "code",
   "source": "sample_size = 5",
   "id": "3ff7d92d88aef19e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T09:28:01.086999Z",
     "start_time": "2024-11-08T09:28:00.688456Z"
    }
   },
   "cell_type": "code",
   "source": "original_dataset_tokenized = prepare_dataset(dataset=dataset, tokenizer=tokenizer, model=model, sample_size=sample_size)",
   "id": "7f41a2fe2aa324e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ebf8766d0754642a632ef97a0a5fffc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47ac97c6728241bfaca8155389245f2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T23:15:31.699756Z",
     "start_time": "2024-11-07T23:15:31.486927Z"
    }
   },
   "cell_type": "code",
   "source": "paraphrased_dataset_tokenized = prepare_dataset(dataset=paraphrased_dataset, tokenizer=tokenizer, model=model, sample_size=sample_size)",
   "id": "b052b67bbef85bd1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizing and reformatting instruction data:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53c64fa667f9419e9c9555e07d3e1dc0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d5890d5ada4498baf0aa017f6a1f61a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T23:15:42.739900Z",
     "start_time": "2024-11-07T23:15:38.184535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gradients = dict()\n",
    "\n",
    "for original in original_dataset_tokenized:\n",
    "    original_id = original[\"id\"][0][0]\n",
    "    \n",
    "    original_gradients = get_gradients(model, original, device)    \n",
    "    original_flattened_gradients = get_flattened_weight_vector(original_gradients)\n",
    "    \n",
    "    for paraphrased in paraphrased_dataset_tokenized:\n",
    "        paraphrased_id = paraphrased[\"id\"][0][0]\n",
    "        \n",
    "        paraphrased_gradients = get_gradients(model, paraphrased, device)\n",
    "        paraphrased_flattened_gradients = get_flattened_weight_vector(paraphrased_gradients)\n",
    "        \n",
    "        gradients[(original_id, paraphrased_id)] = original_flattened_gradients.dot(paraphrased_flattened_gradients)"
   ],
   "id": "a0577aca6eaee9a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukashinterleitner/anaconda3/envs/master-thesis/lib/python3.12/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (770) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 770].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m paraphrased \u001B[38;5;129;01min\u001B[39;00m paraphrased_dataset_tokenized:\n\u001B[1;32m     10\u001B[0m     paraphrased_id \u001B[38;5;241m=\u001B[39m paraphrased[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 12\u001B[0m     paraphrased_gradients \u001B[38;5;241m=\u001B[39m get_gradients(model, paraphrased, device)\n\u001B[1;32m     13\u001B[0m     paraphrased_flattened_gradients \u001B[38;5;241m=\u001B[39m get_flattened_weight_vector(paraphrased_gradients)\n\u001B[1;32m     15\u001B[0m     gradients[(original_id, paraphrased_id)] \u001B[38;5;241m=\u001B[39m original_flattened_gradients\u001B[38;5;241m.\u001B[39mdot(paraphrased_flattened_gradients)\n",
      "File \u001B[0;32m~/Documents/Master_Data_Science/Master_Thesis/code/utilities/gradient_operations.py:10\u001B[0m, in \u001B[0;36mget_gradients\u001B[0;34m(model, batch, device)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# set gradients to zero, so that gradients to not accumulate for each iteration\u001B[39;00m\n\u001B[1;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 10\u001B[0m output \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device), attention_mask\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device), use_cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     13\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/envs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/anaconda3/envs/master-thesis/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1357\u001B[0m, in \u001B[0;36mBertLMHeadModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1355\u001B[0m     use_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1357\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbert(\n\u001B[1;32m   1358\u001B[0m     input_ids,\n\u001B[1;32m   1359\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   1360\u001B[0m     token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids,\n\u001B[1;32m   1361\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   1362\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m   1363\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m   1364\u001B[0m     encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[1;32m   1365\u001B[0m     encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[1;32m   1366\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m   1367\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m   1368\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1369\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   1370\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1371\u001B[0m )\n\u001B[1;32m   1373\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1374\u001B[0m prediction_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls(sequence_output)\n",
      "File \u001B[0;32m~/anaconda3/envs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/master-thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/anaconda3/envs/master-thesis/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1073\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1072\u001B[0m     buffered_token_type_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39mtoken_type_ids[:, :seq_length]\n\u001B[0;32m-> 1073\u001B[0m     buffered_token_type_ids_expanded \u001B[38;5;241m=\u001B[39m buffered_token_type_ids\u001B[38;5;241m.\u001B[39mexpand(batch_size, seq_length)\n\u001B[1;32m   1074\u001B[0m     token_type_ids \u001B[38;5;241m=\u001B[39m buffered_token_type_ids_expanded\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The expanded size of the tensor (770) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 770].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T21:06:14.474692Z",
     "start_time": "2024-11-07T21:06:14.459918Z"
    }
   },
   "cell_type": "code",
   "source": "gradients",
   "id": "645933176121d9fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('lima_0', 'lima_0'): 0,\n",
       " ('lima_0', 'lima_1'): 0,\n",
       " ('lima_0', 'lima_2'): 0,\n",
       " ('lima_0', 'lima_3'): 0,\n",
       " ('lima_0', 'lima_4'): 0,\n",
       " ('lima_1', 'lima_0'): 0,\n",
       " ('lima_1', 'lima_1'): 0,\n",
       " ('lima_1', 'lima_2'): 0,\n",
       " ('lima_1', 'lima_3'): 0,\n",
       " ('lima_1', 'lima_4'): 0,\n",
       " ('lima_2', 'lima_0'): 0,\n",
       " ('lima_2', 'lima_1'): 0,\n",
       " ('lima_2', 'lima_2'): 0,\n",
       " ('lima_2', 'lima_3'): 0,\n",
       " ('lima_2', 'lima_4'): 0,\n",
       " ('lima_3', 'lima_0'): 0,\n",
       " ('lima_3', 'lima_1'): 0,\n",
       " ('lima_3', 'lima_2'): 0,\n",
       " ('lima_3', 'lima_3'): 0,\n",
       " ('lima_3', 'lima_4'): 0,\n",
       " ('lima_4', 'lima_0'): 0,\n",
       " ('lima_4', 'lima_1'): 0,\n",
       " ('lima_4', 'lima_2'): 0,\n",
       " ('lima_4', 'lima_3'): 0,\n",
       " ('lima_4', 'lima_4'): 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:05:32.927284Z",
     "start_time": "2024-11-06T11:05:32.917267Z"
    }
   },
   "cell_type": "code",
   "source": "#flattened_gradients.size()",
   "id": "b531332c6bc28f41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1176764416])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:05:33.051204Z",
     "start_time": "2024-11-06T11:05:33.044899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# todo: investigate how olmo uses a single training iteration, check masking\n",
    "# todo: add filtering with regard to open instruct (threshold for similarity)\n",
    "# todo: ranking between sampling\n",
    "# todo: tf-idf -> term-frequency inverse-document-frequency\n",
    "# todo: think about explainability vs. similarity"
   ],
   "id": "e1ce277adef4a061",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:05:33.120900Z",
     "start_time": "2024-11-06T11:05:33.114491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# random projections to reduce weight vector size\n",
    "# compare ranking to other algorithms: bm25, tf-idf, (rouge optionally)"
   ],
   "id": "f1b0b36aa660f94e",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
